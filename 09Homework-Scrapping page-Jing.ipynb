{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Billboard Top 100 Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.billboard.com/charts/hot-100')\n",
    "doc = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "rankList = []\n",
    "songList = []\n",
    "artistList = []\n",
    "\n",
    "ranks = doc.find_all(class_='chart-list-item__rank')\n",
    "for rank in ranks:\n",
    "    rankList.append(rank.text.strip())\n",
    "\n",
    "songs = doc.find_all(class_= 'chart-list-item__title-text')\n",
    "for song in songs:\n",
    "    songList.append(song.text.strip())\n",
    "    \n",
    "artists = doc.find_all(class_='chart-list-item__artist')\n",
    "for artist in artists:\n",
    "    artistList.append(artist.text.strip())\n",
    "\n",
    "#OUTPUT CSV\n",
    "total = dict(zip(['RANK', 'SONG', 'ARTIST'],[rankList, songList, artistList]))\n",
    "table = pd.DataFrame(total)\n",
    "table.to_csv(\"BillboardTop100Songs.csv\", index=False)\n",
    "\n",
    "#this website must uses (response.content, 'html.parser')\n",
    "#check the doc before choosing html.parser or text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Science Fiction Books by Female Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.goodreads.com/list/show/6934.Science_Fiction_Books_by_Female_Authors')\n",
    "books = BeautifulSoup(response.text)\n",
    "books = books.find(class_='tableList js-dataTooltip')\n",
    "\n",
    "rankList = []\n",
    "titleList = []\n",
    "authorList = []\n",
    "scoreList = []\n",
    "voteList = []\n",
    "ratingList = []\n",
    "\n",
    "ranks = books.find_all(class_='number')\n",
    "for rank in ranks:\n",
    "    rankList.append(rank.text.strip())\n",
    "\n",
    "titles = books.find_all(class_='bookTitle')\n",
    "for title in titles:\n",
    "    titleList.append(title.text.strip())\n",
    "    \n",
    "authors = books.find_all(attrs={\"itemprop\": \"author\"})\n",
    "for author in authors:\n",
    "    authorList.append(author.text.strip())\n",
    "    \n",
    "rates = books.find_all(class_='minirating')\n",
    "for rate in rates:\n",
    "    ratingList.append(rate.text.strip())\n",
    "    \n",
    "votes = books.find_all(class_='smallText uitext')\n",
    "for vote in votes:\n",
    "    text = vote.text.strip().replace('\\n', '').replace('\\n', '')\n",
    "    #Split the text, find the socre\n",
    "    position = text.find(',              and')\n",
    "    scoreList.append(text[0:position])\n",
    "    #Split the text, find the vulue of vote\n",
    "    voteList.append(text[position+18:-1])\n",
    "\n",
    "#OUTPUT CSV\n",
    "total = dict(zip(['RANK', 'TITLE', 'AUTHOR', 'SCORE', 'VOTES', 'RATING'],[rankList, titleList, authorList, scoreList, voteList, ratingList]))\n",
    "table = pd.DataFrame(total)\n",
    "table.to_csv(\"ScienceFictionBooks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Science Fiction Books by Female Authors (Clean Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.goodreads.com/list/show/6934.Science_Fiction_Books_by_Female_Authors')\n",
    "books = BeautifulSoup(response.text)\n",
    "\n",
    "rankList = []\n",
    "titleList = []\n",
    "seriesList = []\n",
    "seriesNumList = []\n",
    "authorList = []\n",
    "scoreList = []\n",
    "voteList = []\n",
    "ratingList = []\n",
    "ratingNumList = []\n",
    "\n",
    "#RANKS' PART\n",
    "ranks = books.find_all(class_='number')\n",
    "for rank in ranks:\n",
    "    rankList.append(rank.text.strip())\n",
    "\n",
    "#TITLES' PART\n",
    "titles = books.find_all(class_='bookTitle')\n",
    "for title in titles:\n",
    "    title = title.text.strip()\n",
    "    #If the book doesn't belong to any series\n",
    "    if (title.find('(') == -1):\n",
    "        titleList.append(title)\n",
    "        seriesList.append('None')\n",
    "        seriesNumList.append('None')\n",
    "    else:\n",
    "        #Find the position of parentheses\n",
    "        l_position = title.find('(')\n",
    "        if (title.find(', #') != -1): \n",
    "            p_position = title.find(', #')\n",
    "            detract = 3\n",
    "        else:\n",
    "            p_position = title.find('#')\n",
    "            detract = 1\n",
    "        r_position = title.find(')')\n",
    "        #Write them into lists\n",
    "        titleList.append(title[0:l_position].strip())\n",
    "        seriesList.append(title[l_position+1:p_position].strip())\n",
    "        seriesNumList.append(title[p_position+detract:r_position].strip())\n",
    "\n",
    "#AUTHOR'S PART\n",
    "authors = books.find_all(attrs={\"itemprop\": \"author\"})\n",
    "for author in authors:\n",
    "    authorList.append(author.text.strip())\n",
    "\n",
    "#RATES' PART\n",
    "rates = books.find_all(class_='minirating')\n",
    "for rate in rates:\n",
    "    rate = rate.text.strip()\n",
    "    \n",
    "    #If the text includ \"really liked it\", remove it\n",
    "    if (rate.find('really liked it') != -1):\n",
    "        rate = rate.replace('really liked it','').strip()\n",
    "    \n",
    "    #Get the positions\n",
    "    l_position = rate.find(' avg')\n",
    "    c_position = rate.find('â€”')\n",
    "    r_position = rate.rfind(' ratings')\n",
    "    \n",
    "    #Write them into lists\n",
    "    ratingList.append(rate[0:l_position].replace(',',''))\n",
    "    ratingNumList.append(rate[c_position+2:r_position].replace(',',''))\n",
    "\n",
    "#VOTES' PART\n",
    "votes = books.find_all(class_='smallText uitext')\n",
    "for vote in votes:\n",
    "    text = vote.text.replace('\\n', '').replace('\\n', '')\n",
    "    #Split the text, find the socre\n",
    "    position = text.find(',              and')\n",
    "    scoreList.append(text[7:position])\n",
    "    #Split the text, find the vote vulue\n",
    "    preVote = text[position+18:-1]\n",
    "    votePosition = preVote.find(' people voted')\n",
    "    voteList.append(preVote[0:votePosition])\n",
    "\n",
    "#OUTPUT CSV\n",
    "total = dict(zip(['RANK', 'TITLE', 'SERIES', 'NUMBER IN SERIES', 'AUTHOR', 'SCORE', 'AVG RATINGS','NUM OF RATINGS', 'VOTES'],[rankList, titleList, seriesList, seriesNumList, authorList, ratingList, ratingNumList, voteList]))\n",
    "table = pd.DataFrame(total)\n",
    "table.to_csv(\"ScienceFictionBooks_Clean.csv\", index=False)\n",
    "\n",
    "#why the first one Billboard sometimes doesnot produce lists?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epicurious Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set request times\n",
    "times = 3\n",
    "\n",
    "#Set a search keyword\n",
    "keyword = 'cucumbers'\n",
    "\n",
    "#Initialize lists\n",
    "categoryList = []\n",
    "titleList = []\n",
    "summaryList = []\n",
    "ratingList = []\n",
    "againList = []\n",
    "urlList = []\n",
    "\n",
    "#Initialize search results\n",
    "finalResult = \"\"\n",
    "\n",
    "#Start requesting result\n",
    "for i in range(1,times):\n",
    "    \n",
    "    #Get the result list\n",
    "    page = requests.get(\"https://www.epicurious.com/search/\" + keyword + \"?page=\" + str(i))\n",
    "    result = BeautifulSoup(page.text)\n",
    "    result = result.find(attrs={\"data-group-number\": i})\n",
    "    \n",
    "    #Combine all results together\n",
    "    finalResult += str(result)\n",
    "\n",
    "#Search the final result\n",
    "epicurious = BeautifulSoup(finalResult)\n",
    "\n",
    "#Get the categories list\n",
    "categories = epicurious.find_all(class_=\"tag\")\n",
    "for category in categories:\n",
    "    category = category.text.strip()\n",
    "    categoryList.append(category)\n",
    "    \n",
    "#Get the title list\n",
    "titles = epicurious.find_all(attrs={\"itemprop\": \"name\"})\n",
    "for title in titles:\n",
    "    title = title.text.strip()\n",
    "    titleList.append(title)\n",
    "\n",
    "#Get the summary list\n",
    "summaries = epicurious.find_all(class_=\"summary\")\n",
    "for summary in summaries:\n",
    "    summary = summary.find(class_=\"dek\").text\n",
    "    summaryList.append(summary)\n",
    "    \n",
    "#Get the rating list\n",
    "ratings = epicurious.find_all(class_=\"summary\")\n",
    "for rating in ratings:\n",
    "    try:\n",
    "        rating = rating.find(attrs={\"itemprop\": \"ratingValue\"}).text\n",
    "    except:\n",
    "        rating = \"N/A\"\n",
    "    ratingList.append(rating)\n",
    "\n",
    "#Get the \"Make it again\" list\n",
    "agains = epicurious.find_all(class_=\"summary\")\n",
    "for again in agains:\n",
    "    try: \n",
    "        again = again.find(class_=\"make-again-percentage\").text\n",
    "    except:\n",
    "        again = \"N/A\"\n",
    "    againList.append(again)\n",
    "\n",
    "#Get the URL list\n",
    "urls = epicurious.find_all(class_=\"hed\")\n",
    "for url in urls:\n",
    "    url = url.find(\"a\").get('href')\n",
    "    urlList.append(\"https://www.epicurious.com\"+url)\n",
    "      \n",
    "#Output CSV\n",
    "total = dict(\n",
    "    zip([\"Category\", \n",
    "         \"Title\", \n",
    "         \"Summary\", \n",
    "         \"Rating\", \n",
    "         \"Make it again\", \n",
    "         \"URL\"],\n",
    "        [categoryList,\n",
    "         titleList, \n",
    "         summaryList, \n",
    "         ratingList, \n",
    "         againList, \n",
    "         urlList]))\n",
    "table = pd.DataFrame(total)\n",
    "table.to_csv(\"Epicurious.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epicurious Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the category to filter, recipe or article\n",
    "keyword = \"recipe\"\n",
    "\n",
    "#Set the file location\n",
    "file = \"Epicurious.csv\"\n",
    "\n",
    "#Read CSV\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "#Filter \n",
    "results = df[df[\"Category\"] == keyword].values\n",
    "\n",
    "#Initialize lists\n",
    "categoryList = []\n",
    "titleList = []\n",
    "summaryList = []\n",
    "ratingList = []\n",
    "againList = []\n",
    "urlList = []\n",
    "stepList = []\n",
    "ingList = []\n",
    "tagList = []\n",
    "\n",
    "#Featching ingredients, directions and tags\n",
    "for i in results:\n",
    "    page = requests.get(i[5])\n",
    "    result = BeautifulSoup(page.text)\n",
    "    result = result.find(class_=\"recipe-and-additional-content\")\n",
    "    \n",
    "    #Rereade data\n",
    "    categoryList.append(i[0])\n",
    "    titleList.append(i[1])\n",
    "    summaryList.append(i[2])\n",
    "    ratingList.append(i[3])\n",
    "    againList.append(i[4])\n",
    "    urlList.append(i[5])\n",
    "    \n",
    "    #Initialize ingredients(temporary)\n",
    "    t_ingredients = \"\"\n",
    "    #Scrap ingredients\n",
    "    ingredients = result.find(class_=\"ingredients\")\n",
    "    t_ingredients += '\\n'.join([ingredient.text for ingredient in ingredients])\n",
    "    #Write them into ingList\n",
    "    ingList.append(t_ingredients)\n",
    "    \n",
    "    #Initialize steps(temporary)\n",
    "    #I didn't find direction, so guess it should be preparation? \n",
    "    t_steps = \"\"\n",
    "    #Scrap steps\n",
    "    steps = result.find(class_=\"preparation-steps\")\n",
    "    t_steps += \"\\n\".join([step.text.strip() for step in steps])\n",
    "    #Write them into stepList\n",
    "    stepList.append(t_steps)\n",
    "    \n",
    "    #Initialize tags(temporary)\n",
    "    t_tags = \"\"\n",
    "    #Scrap tags\n",
    "    tags = result.find(class_=\"tags\")\n",
    "    t_tags += \",\".join([tag.text.strip() for tag in tags])\n",
    "    #Write them into tagList\n",
    "    tagList.append(t_tags)\n",
    "\n",
    "#Output CSV\n",
    "total = dict(\n",
    "    zip([\"Category\", \n",
    "         \"Title\", \n",
    "         \"Summary\",\n",
    "         \"Rating\", \n",
    "         \"Make it again\", \n",
    "         \"Ingredient\",\n",
    "         \"Step\",\n",
    "         \"Tag\",\n",
    "         \"URL\"],\n",
    "        [categoryList,\n",
    "         titleList, \n",
    "         summaryList, \n",
    "         ratingList, \n",
    "         againList, \n",
    "         ingList,\n",
    "         stepList,\n",
    "         tagList,\n",
    "         urlList]))\n",
    "table = pd.DataFrame(total)\n",
    "table.to_csv(\"Epicurious_Recips_Only.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrolyrics Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a singer page\n",
    "singer = \"Mariah Carey\"\n",
    "\n",
    "#Initialize \n",
    "songList = []\n",
    "yearList = []\n",
    "popList = []\n",
    "urlList = []\n",
    "singerName = singer.replace(\" \",\"-\")\n",
    "\n",
    "#Count pages\n",
    "def countNum (singerName):\n",
    "    lastNum = 0\n",
    "    stopSearch = 1\n",
    "    while stopSearch == 1:\n",
    "        if lastNum == 0:\n",
    "            page = requests.get(\"http://www.metrolyrics.com/\" + singerName +\"-lyrics.html\")\n",
    "            result = BeautifulSoup(page.text)\n",
    "        else:\n",
    "            page = requests.get(\"http://www.metrolyrics.com/\" + singerName +\"-alpage-\" + str(lastNum) + \".html\")\n",
    "            result = BeautifulSoup(page.text)\n",
    "            \n",
    "        #If it's the last page\n",
    "        if (result.find(\"a\", class_=\"button next disabled\") == None):\n",
    "            \n",
    "            #If it's not the last page but including \"...\", get the last num and keep searching\n",
    "            if (result.find(text=\" ... \") != None):\n",
    "                url = result.find(\"span\", \"pages\").find_all(\"a\")\n",
    "                url = url[len(url)-1].get('href')\n",
    "                #find the position\n",
    "                l_position = url.find(\"alpage-\")\n",
    "                r_position = url.find(\".html\")\n",
    "                lastNum = url[l_position+7:r_position]\n",
    "                stopSearch = 1\n",
    "                \n",
    "            #If it's not the last page and not including \"...\", get the last num and stop searching \n",
    "            else:\n",
    "                lastNum = result.find(\"span\", \"pages\").find_all(\"a\")\n",
    "                lastNum = lastNum[len(lastNum)-1].text.strip()\n",
    "                stopSearch = 0\n",
    "                \n",
    "        #If the NEXT button is disabled, get the last num and stop searching\n",
    "        elif (result.find(\"a\", class_=\"button next disabled\") != None) :\n",
    "            lastNum = result.find(\"span\", \"pages\").find_all(\"a\")\n",
    "            lastNum = lastNum[len(lastNum)-1].text.strip()\n",
    "            stopSearch = 0\n",
    "        else:\n",
    "            print(\"Something went wrong!\")\n",
    "            break\n",
    "    return int(lastNum)\n",
    "\n",
    "for i in range(1,countNum(singerName)+1):\n",
    "    #Featch data\n",
    "    page = requests.get(\"http://www.metrolyrics.com/\" + singerName +\"-alpage-\" + str(i) + \".html\")\n",
    "    result = BeautifulSoup(page.text)\n",
    "    result = result.find(class_=\"songs-table compact\")\n",
    "\n",
    "    #Scrap songs' title and URL\n",
    "    songs = result.find_all(class_=\"title\")\n",
    "    for song in songs:\n",
    "        url = song.get('href')\n",
    "        urlList.append(url)\n",
    "        song = song.text.strip()\n",
    "        song = song.replace(\" Lyrics\",\"\")\n",
    "        songList.append(song)\n",
    "\n",
    "    #Scrap years\n",
    "    years = result.find(\"tbody\").find_all(\"tr\")\n",
    "    for year in years:\n",
    "        year = year.find(\"td\").next_sibling.next_sibling.next_sibling.next_sibling.text\n",
    "        yearList.append(year)\n",
    "\n",
    "    #Scrap popularity\n",
    "    pops = result.find(\"tbody\").find_all(\"tr\")\n",
    "    for pop in pops:\n",
    "        pop = pop.find(class_=\"bar\").find(\"span\")\n",
    "        pop = pop[\"style\"].replace(\"width:\",\"\").replace(\"%;\",\"\").strip()\n",
    "        pop = \"{:.0f}\".format(float(pop))\n",
    "        popList.append(pop)\n",
    "\n",
    "#Output CSV\n",
    "total = dict(\n",
    "    zip([\"Song\", \n",
    "         \"Year\", \n",
    "         \"Popularity\",\n",
    "         \"URL\"\n",
    "         ],\n",
    "        [songList,\n",
    "         yearList, \n",
    "         popList, \n",
    "         urlList]))\n",
    "table = pd.DataFrame(total)\n",
    "table.to_csv(\"Metrolyrics-\" + singerName + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrolyrics Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Set the file location\n",
    "file = \"Metrolyrics-Mariah-Carey.csv\"\n",
    "\n",
    "#Read CSV\n",
    "df = pd.read_csv(file) \n",
    "\n",
    "#Only fetch top 5 songs for saving memory\n",
    "results = df.head(5).values\n",
    "\n",
    "#For using the results list, those numbers:\n",
    "    #0 SongName\n",
    "    #1 Year\n",
    "    #2 Popularity\n",
    "    #3 URL\n",
    "\n",
    "#Initialize \n",
    "songList = []\n",
    "yearList = []\n",
    "popList = []\n",
    "urlList = []    \n",
    "lyricList = []\n",
    "\n",
    "#Featching lyrics\n",
    "for i in results: \n",
    "    page = requests.get(i[3])\n",
    "    \n",
    "    #Rereade data\n",
    "    songList.append(i[0])\n",
    "    yearList.append(i[1])\n",
    "    popList.append(i[2])\n",
    "    urlList.append(i[3])\n",
    "    \n",
    "    #Read the URL\n",
    "    result = BeautifulSoup(page.text)\n",
    "    \n",
    "    #Read lyric\n",
    "    t_lyric = \"\"\n",
    "    lines = result.find_all(class_=\"verse\")\n",
    "    t_lyric = \"\\n\".join([line.text for line in lines])\n",
    "    lyricList.append(t_lyric)\n",
    "\n",
    "#Output CSV\n",
    "total = dict(\n",
    "    zip([\"Song\", \n",
    "         \"Year\", \n",
    "         \"Popularity\",\n",
    "         \"Lyric\",\n",
    "         \"URL\"],\n",
    "        [songList,\n",
    "         yearList, \n",
    "         popList, \n",
    "         lyricList,\n",
    "         urlList]))\n",
    "table = pd.DataFrame(total)\n",
    "table.to_csv(file.replace(\".csv\", \"-lyrics.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
